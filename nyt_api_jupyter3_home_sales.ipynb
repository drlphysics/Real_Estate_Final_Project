{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress loaded from nyt_progress_home_sales.json: {'query': '\"home sales\"', 'page': 15, 'request_count': 15, 'date': '2024-08-19'}\n",
            "Processing query: \"home sales\" starting from page 15\n",
            "Data saved to nyt_home_sales_articles_20220101_20240731.csv\n",
            "Total articles fetched for query '\"home sales\"': 376\n",
            "\n",
            "DataFrame for '\"home sales\"':\n",
            "                Title               URL        Lead Paragraph  \\\n",
            "0  No title available  No URL available  No content available   \n",
            "1  No title available  No URL available  No content available   \n",
            "2  No title available  No URL available  No content available   \n",
            "3  No title available  No URL available  No content available   \n",
            "4  No title available  No URL available  No content available   \n",
            "\n",
            "    Publication Date               Source  \n",
            "0  No date available  No source available  \n",
            "1  No date available  No source available  \n",
            "2  No date available  No source available  \n",
            "3  No date available  No source available  \n",
            "4  No date available  No source available  \n",
            "All queries have been processed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import requests  # Ensure that the requests module is imported\n",
        "\n",
        "# Constants for rate limits\n",
        "REQUESTS_PER_MINUTE = 5\n",
        "SECONDS_PER_REQUEST = 12\n",
        "REQUESTS_PER_DAY = 500\n",
        "\n",
        "# Function to generate a unique progress filename for each query\n",
        "def get_progress_filename(query):\n",
        "    # Remove any unwanted characters such as quotes and replace spaces with underscores\n",
        "    query_safe = query.replace('\"', '').replace(\"'\", \"\").replace(' ', '_').lower()\n",
        "    return f\"nyt_progress_{query_safe}.json\"\n",
        "\n",
        "# Function to generate a unique CSV filename for each query\n",
        "def get_csv_filename(query):\n",
        "    # Remove any unwanted characters such as quotes and replace spaces with underscores\n",
        "    query_safe = query.replace('\"', '').replace(\"'\", \"\").replace(' ', '_').lower()\n",
        "    return f\"nyt_{query_safe}_articles_20220101_20240731.csv\"\n",
        "\n",
        "# Save the progress to a unique file for each query\n",
        "def save_progress(query, page, request_count):\n",
        "    progress_filename = get_progress_filename(query)\n",
        "    progress_data = {\n",
        "        \"query\": query,\n",
        "        \"page\": page,\n",
        "        \"request_count\": request_count,\n",
        "        \"date\": datetime.now().strftime('%Y-%m-%d')  # Save the current date\n",
        "    }\n",
        "    with open(progress_filename, \"w\") as f:\n",
        "        json.dump(progress_data, f)\n",
        "    print(f\"Progress saved to {progress_filename}: {progress_data}\")\n",
        "\n",
        "# Load the progress from a unique file for each query\n",
        "def load_progress(query):\n",
        "    progress_filename = get_progress_filename(query)\n",
        "    if not os.path.exists(progress_filename):\n",
        "        print(f\"No progress file found for query '{query}'. Starting from the beginning.\")\n",
        "        return None\n",
        "    \n",
        "    with open(progress_filename, \"r\") as f:\n",
        "        progress_data = json.load(f)\n",
        "        print(f\"Progress loaded from {progress_filename}: {progress_data}\")\n",
        "        return progress_data\n",
        "\n",
        "def get_nyt_article_count(api_key, query, begin_date=None, end_date=None):\n",
        "    url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
        "    \n",
        "    params = {\n",
        "        'q': query,\n",
        "        'api-key': api_key,\n",
        "        'begin_date': begin_date,\n",
        "        'end_date': end_date,\n",
        "        'page': 0,  # Only need to request the first page to get the count\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)  # Use the requests module here\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    \n",
        "    # Get the total number of articles from the 'meta' field\n",
        "    total_articles = data['response']['meta']['hits']\n",
        "    \n",
        "    return total_articles\n",
        "\n",
        "def get_nyt_articles(api_key, query, begin_date=None, end_date=None, page=0):\n",
        "    url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
        "    \n",
        "    params = {\n",
        "        'q': query,\n",
        "        'api-key': api_key,\n",
        "        'page': page,\n",
        "        'begin_date': begin_date,\n",
        "        'end_date': end_date,\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)  # Use the requests module here\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    articles = data['response']['docs']\n",
        "    \n",
        "    return articles\n",
        "\n",
        "def process_query(api_key, query, begin_date, end_date, request_count, max_requests_per_day, page=0):\n",
        "    print(f\"Processing query: {query} starting from page {page}\")\n",
        "    \n",
        "    all_articles = []\n",
        "    total_articles_fetched = 0  # Counter to track the total number of articles fetched\n",
        "    \n",
        "    # Generate the CSV filename for the query\n",
        "    csv_filename = get_csv_filename(query)\n",
        "    \n",
        "    # Try loading existing data from the CSV file if it exists (to avoid overwriting)\n",
        "    if os.path.exists(csv_filename):\n",
        "        existing_df = pd.read_csv(csv_filename)\n",
        "        all_articles = existing_df.to_dict('records')  # Convert to list of dicts\n",
        "        total_articles_fetched = len(all_articles)  # Initialize counter with existing data\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            # Check if we've hit the daily request limit\n",
        "            if request_count >= max_requests_per_day:\n",
        "                print(f\"Reached the daily request limit of {max_requests_per_day}. Stopping.\")\n",
        "                save_progress(query, page, request_count)\n",
        "                # Save the data collected so far\n",
        "                save_to_csv(all_articles, csv_filename)\n",
        "                print(f\"Total articles extracted so far: {total_articles_fetched}\")\n",
        "                return all_articles, request_count\n",
        "\n",
        "            # Fetch articles for the current page\n",
        "            articles = get_nyt_articles(api_key, query, begin_date, end_date, page=page)\n",
        "            \n",
        "            if not articles:\n",
        "                break\n",
        "            \n",
        "            all_articles.extend(articles)\n",
        "            total_articles_fetched += len(articles)  # Update the total articles counter\n",
        "            print(f\"Fetched {len(articles)} articles for this page, total so far: {total_articles_fetched}\")\n",
        "            \n",
        "            page += 1\n",
        "            request_count += 1  # Increment request count across queries\n",
        "            \n",
        "            # Save progress after every successful request\n",
        "            save_progress(query, page, request_count)\n",
        "            \n",
        "            # Periodically save the data to the CSV file to ensure no data loss\n",
        "            save_to_csv(all_articles, csv_filename)\n",
        "            \n",
        "            # Enforce rate limit: sleep for 12 seconds between requests\n",
        "            time.sleep(SECONDS_PER_REQUEST)\n",
        "        \n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                print(f\"Rate limit exceeded. Backing off...\")\n",
        "                time.sleep(60)\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    # Final save after query is fully processed\n",
        "    save_to_csv(all_articles, csv_filename)\n",
        "    print(f\"Total articles fetched for query '{query}': {total_articles_fetched}\")\n",
        "    \n",
        "    # Create a unique DataFrame for the query\n",
        "    query_df = create_dataframe(all_articles)\n",
        "    \n",
        "    return query_df, request_count\n",
        "\n",
        "def create_dataframe(all_articles):\n",
        "    # Convert the list of articles to a DataFrame\n",
        "    data = {\n",
        "        'Title': [article.get('headline', {}).get('main', 'No title available') for article in all_articles],\n",
        "        'URL': [article.get('web_url', 'No URL available') for article in all_articles],\n",
        "        'Lead Paragraph': [article.get('lead_paragraph', 'No content available') for article in all_articles],\n",
        "        'Publication Date': [article.get('pub_date', 'No date available') for article in all_articles],\n",
        "        'Source': [article.get('source', 'No source available') for article in all_articles],\n",
        "    }\n",
        "\n",
        "    nyt_extract_df = pd.DataFrame(data)\n",
        "    \n",
        "    return nyt_extract_df\n",
        "\n",
        "def save_to_csv(all_articles, csv_filename):\n",
        "    nyt_extract_df = create_dataframe(all_articles)\n",
        "    \n",
        "    # Save the DataFrame to CSV file\n",
        "    nyt_extract_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "def main():\n",
        "    # Your API key\n",
        "    api_key = 'sample_api_key'  # Replace with your own API key\n",
        "    \n",
        "    # Define the date range\n",
        "    begin_date = '20220101'  # Start date in YYYYMMDD format\n",
        "    end_date = '20240731'    # End date in YYYYMMDD format\n",
        "    \n",
        "    # List of queries to process\n",
        "    queries = ['\"home sales\"']\n",
        "    \n",
        "    # Dictionary to store the DataFrame for each query\n",
        "    query_dataframes = {}\n",
        "    \n",
        "    # Iterate over the queries\n",
        "    for query in queries:\n",
        "        # Load progress for the specific query\n",
        "        progress = load_progress(query)\n",
        "        if progress:\n",
        "            last_query = progress['query']\n",
        "            start_page = progress['page']\n",
        "            request_count = progress['request_count']\n",
        "            last_run_date = progress['date']\n",
        "            \n",
        "            # If a new day has started, reset the request count\n",
        "            if last_run_date != datetime.now().strftime('%Y-%m-%d'):\n",
        "                print(\"New day detected. Resetting the request count.\")\n",
        "                request_count = 0\n",
        "                start_page = 0\n",
        "        else:\n",
        "            start_page = 0\n",
        "            request_count = 0\n",
        "\n",
        "        # Process the query and store the DataFrame in the dictionary\n",
        "        query_df, request_count = process_query(api_key, query, begin_date, end_date, request_count, REQUESTS_PER_DAY, page=start_page)\n",
        "        query_dataframes[query] = query_df\n",
        "        \n",
        "        # Display the DataFrame for the current query\n",
        "        print(f\"\\nDataFrame for '{query}':\")\n",
        "        print(query_df.head())\n",
        "\n",
        "    print(\"All queries have been processed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'query_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mquery_df\u001b[49m(head)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'query_df' is not defined"
          ]
        }
      ],
      "source": [
        "query_df(head)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
